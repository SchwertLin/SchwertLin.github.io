<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="基于强化学习的Transformer目标检测模型框架（RL-DETR）模型方法本工作提出一种融合强化学习思想的Transformer目标检测框架，旨在提升标准DETR架构中对象查询（object queries）更新策略的表达能力与泛化性能。我们将DETR的每一层解码器视为强化学习中的一个时间步（time step），通过建模状态（state）、动作（action）和奖励（reward），引入高斯">
<meta property="og:type" content="article">
<meta property="og:title" content="2025-5-12">
<meta property="og:url" content="http://example.com/2025/03/12/2025-5-12/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="基于强化学习的Transformer目标检测模型框架（RL-DETR）模型方法本工作提出一种融合强化学习思想的Transformer目标检测框架，旨在提升标准DETR架构中对象查询（object queries）更新策略的表达能力与泛化性能。我们将DETR的每一层解码器视为强化学习中的一个时间步（time step），通过建模状态（state）、动作（action）和奖励（reward），引入高斯">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-03-12T07:33:33.000Z">
<meta property="article:modified_time" content="2025-05-15T09:14:45.000Z">
<meta property="article:author" content="Schwertlilien">
<meta property="article:tag" content="目标检测">
<meta name="twitter:card" content="summary"><title>2025-5-12 - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>2025-5-12</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2025-03-12</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2025-05-15</time></div>
<div class="post-categories"><i class="icon fa-regular fa-folder-open" title="分类"></i><a class="post-category" href="/categories/%E5%B7%A5%E4%BD%9C/">工作</a></div>
<div class="post-tags"><i class="icon fa-solid fa-tags" title="标签"></i><a class="post-tag" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约1.1K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><h3 id="基于强化学习的Transformer目标检测模型框架（RL-DETR）"><a href="#基于强化学习的Transformer目标检测模型框架（RL-DETR）" class="headerlink" title="基于强化学习的Transformer目标检测模型框架（RL-DETR）"></a>基于强化学习的Transformer目标检测模型框架（RL-DETR）</h3><h4 id="模型方法"><a href="#模型方法" class="headerlink" title="模型方法"></a>模型方法</h4><p>本工作提出一种融合强化学习思想的Transformer目标检测框架，旨在提升标准DETR架构中对象查询（object queries）更新策略的表达能力与泛化性能。我们将DETR的每一层解码器视为强化学习中的一个时间步（time step），通过建模状态（state）、动作（action）和奖励（reward），引入高斯策略建模（Gaussian Policy）以及基于GPRO的优化函数，引导Transformer更有效地学习目标位置与类别信息。</p>
<h4 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h4><p>整个模型由以下三部分组成：</p>
<ol>
<li><p><strong>图像编码模块</strong>：输入图像通过预训练的ViT模型提取图像特征，输出 $x \in \mathbb{R}^{H \times W \times d}$，其中 $d$ 为特征维度。</p>
</li>
<li><p><strong>Transformer编码器-解码器结构</strong>：包括6层标准编码器和6层解码器，编码器提取全局语义信息，解码器迭代更新对象查询，最终输出边界框和类别。</p>
</li>
<li><p><strong>RL增强查询更新机制</strong>：解码器每层对查询的更新通过高斯采样实现，动作具有随机性；每一层的中间预测均计算奖励以用于强化学习训练。</p>
</li>
</ol>
<h4 id="状态定义"><a href="#状态定义" class="headerlink" title="状态定义"></a>状态定义</h4><p>在第 $k$ 层解码器中，状态定义为该层的对象查询：</p>
<script type="math/tex; mode=display">
  s_k = q_k \in \mathbb{R}^{N \times d},</script><p>其中 $N$ 是对象查询的数量，$d$ 是特征维度，$q_k$ 表示第 $k$ 层解码器的查询输入，是从 $q_{k-1}$ 经过处理后得到的。</p>
<h4 id="动作定义"><a href="#动作定义" class="headerlink" title="动作定义"></a>动作定义</h4><p>动作是对当前查询 $q_k$ 的更新过程，即：</p>
<script type="math/tex; mode=display">
  a_k: q_k \longrightarrow q_{k+1},</script><p>不同于传统DETR中确定性更新，我们引入高斯策略，将动作表示为一个从高斯分布中采样的随机变量：</p>
<script type="math/tex; mode=display">
  q_{k+1} \sim \mathcal{N}(\mu_k, \Sigma_k),</script><p>其中</p>
<script type="math/tex; mode=display">
\mu_k = W_{\mu} h_k, \quad \log \sigma_k^2 = W_{\sigma} h_k,</script><p>$h_k \in \mathbb{R}^{N \times d}$ 是通过标准解码器结构（包括自注意力、交叉注意力和前馈网络）对查询进行处理后的中间特征表示：</p>
<script type="math/tex; mode=display">
  h_k = \text{FFN}(\text{CrossAttn}(\text{SelfAttn}(q_k), x)).</script><p>从而每个位置的下一步查询由如下方式采样：</p>
<script type="math/tex; mode=display">
  q_{k+1} = \mu_k + \epsilon_k \cdot \sigma_k, \quad \epsilon_k \sim \mathcal{N}(0, I).</script><p>这种方式引入了动作的概率建模，支持策略梯度优化。</p>
<h4 id="奖励设计"><a href="#奖励设计" class="headerlink" title="奖励设计"></a>奖励设计</h4><p>每一层输出的对象查询 $q_{k+1}$ 会生成边界框和类别预测，通过以下方式与真实标签比较并计算奖励：</p>
<ol>
<li>使用 Hungarian 匹配算法匹配预测与真实目标；</li>
<li>计算预测框与真实框的平均 IoU：$\text{mean}(\text{IoU}_k)$；</li>
<li>计算分类准确率 $\text{accuracy}_k$。</li>
</ol>
<p>定义奖励为：</p>
<script type="math/tex; mode=display">
  r_k = \text{mean}(\text{IoU}_k) + \alpha \cdot \text{accuracy}_k,</script><p>其中 $\alpha$ 为平衡系数，控制IoU与分类准确度的权重。</p>
<p>或使用负损失作为奖励（用于策略梯度）：</p>
<script type="math/tex; mode=display">
  r_k = - \left( \text{box\_loss}_k + \text{class\_loss}_k \right).</script><h4 id="强化学习优化目标"><a href="#强化学习优化目标" class="headerlink" title="强化学习优化目标"></a>强化学习优化目标</h4><p>为了更稳定地更新策略，我们引入 GPRO（Generalized PPO with Reference Policy）优化目标，具体定义如下：</p>
<script type="math/tex; mode=display">
\mathcal{J}_{\text{GPRO}}(\theta)=\mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(o|q)}
\left\{\frac{1}{G} \sum_{i=1}^{G}\left[\min\left(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}A_i, \text{clip}\left(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}, 1 - \epsilon, 1 + \epsilon \right) A_i\right) - \beta \mathbb{D}_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}})\right]\right\},</script><p>其中：</p>
<ul>
<li>$\pi_{\theta}(o|q)$ 为当前策略生成的动作分布；</li>
<li>$\pi_{\theta_{\text{old}}}$ 为旧策略；</li>
<li>$\pi_{\text{ref}}$ 为参考策略（如标准DETR的确定性更新）；</li>
<li>$A_i$ 为优势函数，定义为：</li>
</ul>
<script type="math/tex; mode=display">
  A_i = r_k^i + \gamma V(q_{k+1}^i) - V(q_k^i),</script><ul>
<li>$\epsilon$ 控制策略更新范围，$\beta$ 控制与参考策略的KL散度正则。</li>
</ul>
<p>该目标通过约束更新步幅与参考策略距离，确保稳定训练，并鼓励在合理范围内探索。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>本模型将DETR解码器过程重新建模为强化学习中的多步决策问题，借助高斯策略建模查询更新过程，并以GPRO作为训练目标，稳定有效地提升了目标检测的性能，增强了模型对复杂查询空间的探索能力。</p>
</div><div class="post-end"><div class="post-prev"><a href="/2025/03/13/2025-3-10-%E4%B8%8D%E6%89%93%E7%AE%97%E4%BD%BF%E7%94%A8KD/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2025/03/10/2025-3-10/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84Transformer%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6%EF%BC%88RL-DETR%EF%BC%89"><span class="toc-content-number">1.</span> <span class="toc-content-text">基于强化学习的Transformer目标检测模型框架（RL-DETR）</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95"><span class="toc-content-number">1.1.</span> <span class="toc-content-text">模型方法</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-content-number">1.2.</span> <span class="toc-content-text">整体结构</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E7%8A%B6%E6%80%81%E5%AE%9A%E4%B9%89"><span class="toc-content-number">1.3.</span> <span class="toc-content-text">状态定义</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%B9%89"><span class="toc-content-number">1.4.</span> <span class="toc-content-text">动作定义</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E5%A5%96%E5%8A%B1%E8%AE%BE%E8%AE%A1"><span class="toc-content-number">1.5.</span> <span class="toc-content-text">奖励设计</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-content-number">1.6.</span> <span class="toc-content-text">强化学习优化目标</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-content-number">1.7.</span> <span class="toc-content-text">小结</span></a></li></ol></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2025 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>