<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="大模型面试60问 第1章　大语言模型简介Q1：仅编码器（BERT类）、仅解码器（GPT类）和完整的编码器-解码器架构各有什么优缺点？Q2：自注意力机制如何使大模型能够捕捉长距离依赖关系，它跟RNN有什么区别？Q3：大模型为什么有上下文长度的概念？为什么它是指输入和输出的总长度？ 第2章　词元和嵌入Q4：大模型的分词器和传统的中文分词有什么区别？对于指定的词表，一句话是不是只有唯一的分词方式？Q5：">
<meta property="og:type" content="article">
<meta property="og:title" content="2025-7-5">
<meta property="og:url" content="http://example.com/2025/07/05/2025-7-5/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="大模型面试60问 第1章　大语言模型简介Q1：仅编码器（BERT类）、仅解码器（GPT类）和完整的编码器-解码器架构各有什么优缺点？Q2：自注意力机制如何使大模型能够捕捉长距离依赖关系，它跟RNN有什么区别？Q3：大模型为什么有上下文长度的概念？为什么它是指输入和输出的总长度？ 第2章　词元和嵌入Q4：大模型的分词器和传统的中文分词有什么区别？对于指定的词表，一句话是不是只有唯一的分词方式？Q5：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250705183123321.png">
<meta property="article:published_time" content="2025-07-05T06:59:00.000Z">
<meta property="article:modified_time" content="2025-07-05T11:58:42.000Z">
<meta property="article:author" content="Schwertlilien">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250705183123321.png"><title>2025-7-5 - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>2025-7-5</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2025-07-05</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2025-07-05</time></div>

</div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约3.0K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><h1 id="大模型面试60问"><a href="#大模型面试60问" class="headerlink" title="大模型面试60问"></a>大模型面试60问</h1><p><img src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250705183123321.png" alt="image-20250705183123321"></p>
<h2 id="第1章-大语言模型简介"><a href="#第1章-大语言模型简介" class="headerlink" title="第1章　大语言模型简介"></a>第1章　大语言模型简介</h2><p>Q1：仅编码器（BERT类）、仅解码器（GPT类）和完整的编码器-解码器架构各有什么优缺点？<br>Q2：自注意力机制如何使大模型能够捕捉长距离依赖关系，它跟RNN有什么区别？<br>Q3：大模型为什么有上下文长度的概念？为什么它是指输入和输出的总长度？</p>
<h2 id="第2章-词元和嵌入"><a href="#第2章-词元和嵌入" class="headerlink" title="第2章　词元和嵌入"></a>第2章　词元和嵌入</h2><p>Q4：大模型的分词器和传统的中文分词有什么区别？对于指定的词表，一句话是不是只有唯一的分词方式？<br>Q5：大模型是如何区分聊天历史中用户说的话和AI说的话的？<br>Q6：传统的静态词嵌入（如word2vec）与大模型产生的上下文相关的嵌入相比，有什么区别？有了与上下文相关的嵌入，静态词嵌入还有什么价值？<br>Q7：在word2vec等词嵌入空间中，存在king-man + woman ≈ queen的现象，这是为什么？大模型的词元嵌入空间是否也有类似的属性？</p>
<h2 id="第3章-LLM的内部机制"><a href="#第3章-LLM的内部机制" class="headerlink" title="第3章　LLM的内部机制"></a>第3章　LLM的内部机制</h2><p>Q8：注意力机制是如何计算上下文各个词元之间的相关性的？每个注意力头只关注一个词元吗？<br>Q9：如果需要通过修改尽可能少的参数值，让模型忘记某一特定知识，应该修改注意力层还是前馈神经网络层的参数？<br>Q10：为什么注意力机制需要多个头？跟简单地减少注意力头的数量相比，多查询注意力和分组查询注意力优化有什么不同？它们优化的是训练阶段还是推理阶段？<br>Q11：Flash Attention并不能减少计算量，为什么能提升推理速度？Flash Attention是如何实现增量计算softmax的？<br>Q12：跟原始Transformer论文中的绝对位置编码相比，RoPE（旋转位置嵌入）有什么优点？RoPE在长上下文外推时会面临什么挑战？</p>
<h2 id="第4章-文本分类"><a href="#第4章-文本分类" class="headerlink" title="第4章　文本分类"></a>第4章　文本分类</h2><p>Q13：在本章中，嵌入模型+逻辑回归的分类方式获得了0.85的F1分数，而零样本分类方式获得了0.78的F1分数。如果有标注数据，什么情况下会选择零样本分类？<br>Q14：与BERT的掩蔽策略相比，掩码语言建模有何不同？这种预训练方式如何帮助模型在下游的文本分类任务中获得更好的性能？<br>Q15：假设你有一个包含100万条客户评论的数据集，但只有1000条带有标签的数据，请同时利用有标签和无标签的数据，结合表示模型和生成模型的优势，构建一个分类系统。</p>
<h2 id="第5章-文本聚类和主题建模"><a href="#第5章-文本聚类和主题建模" class="headerlink" title="第5章　文本聚类和主题建模"></a>第5章　文本聚类和主题建模</h2><p>Q16：有了强大的生成式大模型，嵌入模型还有什么用？（提示：推荐系统）<br>Q17：词袋法和文档嵌入在实现原理上有什么区别？词袋法是不是一无是处了？<br>Q18：BERTopic中的c-TF-IDF与传统的TF-IDF有何不同？这种差异如何帮助改进主题表示的质量？<br>Q19：基于质心和基于密度的文本聚类算法有什么优缺点？<br>Q20：在一个主题建模项目中，你发现生成的主题中有大量重叠的关键词，如何使用本章介绍的技术来提高主题之间的区分度？</p>
<h2 id="第6章-提示工程"><a href="#第6章-提示工程" class="headerlink" title="第6章　提示工程"></a>第6章　提示工程</h2><p>Q21：针对翻译类、创意写作类、头脑风暴类任务，分别如何设置temperature和top_p？<br>Q22：一个专业的提示词模板由哪几部分构成？为什么提示词中需要描述角色定义？<br>Q23：为了尽可能防止提示词注入，如何设计提示词模板？如何在系统层面检测提示词注入攻击？<br>Q24：在没有推理模型之前，如何让模型先思考后回答？思维链、自洽性、思维树等几种技术各有什么优缺点？<br>Q25：如何保证模型的输出一定是合法的JSON格式？将大模型用于分类任务时，如何保证其输出一定是几个类别之一，而不会输出无关内容？如果开发一个学习英语的应用，如何确保其输出的语言始终限定在指定的词汇表中？</p>
<h2 id="第7章-高级文本生成技术与工具"><a href="#第7章-高级文本生成技术与工具" class="headerlink" title="第7章　高级文本生成技术与工具"></a>第7章　高级文本生成技术与工具</h2><p>Q26：如果我们需要生成小说的标题、角色描述和故事梗概，当单次模型调用生成效果不佳时，如何分步生成？<br>Q27：如果用户跟模型对话轮次过多，超出了模型的上下文限制，但我们又希望尽可能保留用户的对话信息，该怎么办？<br>Q28：如何编写一个智能体，帮助用户规划一次包含机票预订、酒店安排和景点游览的旅行？需要配置哪些工具？如何确保系统在面对不完整或矛盾的信息时仍能提供合理建议？<br>Q29：如果单一智能体的提示词过长，导致性能下降，如何将其拆分为多个智能体，并在合适的时机调用不同的智能体？</p>
<h2 id="第8章-语义搜索与RAG"><a href="#第8章-语义搜索与RAG" class="headerlink" title="第8章　语义搜索与RAG"></a>第8章　语义搜索与RAG</h2><p>Q30：在RAG中，为什么要把文档划分成多个块进行索引？如何解决文档分块后内容上下文缺失的问题？如何处理跨片段的依赖关系？<br>Q31：向量相似度检索不能实现关键词的精确匹配，基于倒排索引的关键词检索不能匹配语义相近的词，如何解决这对矛盾？为什么需要重排序模型？<br>Q32：为什么要在向量相似度检索前，对用户输入的话进行改写？<br>Q33：如果需要根据某长篇小说的内容回答问题，而小说的长度远远超出了上下文限制，应该如何综合利用摘要和RAG技术，使其能同时回答故事梗概和故事细节？</p>
<h2 id="第9章-多模态LLM"><a href="#第9章-多模态LLM" class="headerlink" title="第9章　多模态LLM"></a>第9章　多模态LLM</h2><p>Q34：在CLIP训练过程中，为什么需要同时最大化匹配图文对的相似度和最小化非匹配图文对的相似度？<br>Q35：BLIP-2为何不直接将视觉编码器的输出连接到语言模型，而要引入Q-Former这一中间层结构？<br>Q36：现有一个能力较弱的多模态模型和一个能力较强的文本模型（如DeepSeek-R1），如何结合两者的能力来回答与多模态相关的问题？<br>Q37：如何构建一个AI照片助手，能够对用户的上万张照片进行索引，根据用户的查询高效地检索相关照片？</p>
<h2 id="第10章-构建文本嵌入模型"><a href="#第10章-构建文本嵌入模型" class="headerlink" title="第10章　构建文本嵌入模型"></a>第10章　构建文本嵌入模型</h2><p>Q38：相比交叉编码器，为什么双编码器在大规模相似度搜索中更受欢迎？<br>Q39：在训练嵌入模型时，MNR（多负例排序）损失、余弦相似度损失和softmax损失各有哪些优缺点？在哪些场景下，余弦相似度损失可能比MNR损失更合适？<br>Q40：如何生成负例以提升模型性能？如何构建高质量的难负例？<br>Q41：为什么TSDAE选择使用特殊词元而非平均池化作为句子表征？<br>Q42：相比STSB，MTEB有哪些改进？其中包括哪些类别的嵌入任务？</p>
<h2 id="第11章-为分类任务微调表示模型"><a href="#第11章-为分类任务微调表示模型" class="headerlink" title="第11章　为分类任务微调表示模型"></a>第11章　为分类任务微调表示模型</h2><p>Q43：如果标注的训练数据很少，如何扩增训练数据的数量？（提示：SetFit）<br>Q44：在继续预训练时，如何在保证模型获得特定领域知识的同时，最大限度地保留其通用能力？<br>Q45：请比较以下三种方案在医疗领域文本分类任务上的优缺点：(a)直接使用通用BERT模型微调；(b)在医疗文本上继续预训练BERT后再微调；(c)从头开始用医疗文本预训练模型再微调。<br>Q46：在命名实体识别任务中，当BERT将单词拆分成多个词元时，如何解决标签对齐问题？<br>Q47：假设一个嵌入模型的训练语料主要由英文构成，在中文任务上表现不佳，如何用较低的继续预训练成本提升其中文能力？</p>
<h2 id="第12章-微调生成模型"><a href="#第12章-微调生成模型" class="headerlink" title="第12章　微调生成模型"></a>第12章　微调生成模型</h2><p>Q48：有人声称一篇文章是用DeepSeek-R1生成的，并给了你生成所用的完整提示词，如何证实或证伪这个说法？（提示：利用困惑度）<br>Q49：如何微调一个Llama开源模型，使其输出风格更简洁、更像微信聊天，并保证输出的内容符合国内的大模型安全要求？<br>Q50：QLoRA中的分块量化如何解决普通量化导致的信息损失问题？<br>Q51：现有一个由若干篇文章组成的企业知识库，如何将其转换成适合SFT的数据集？<br>Q52：PPO和DPO相比有什么优缺点？<br>Q53：在PPO中，如何防止模型在微调数据集以外的问题上泛化能力下降？如何防止模型收敛到单一类型的高奖励回答？<br>Q54：设想一个网站上都是AI生成的内容，我们统计了每篇文章的平均用户停留时长，如何将其转化为DPO所需的偏好数据？对于小红书和知乎两种类型的网站，处理方式有什么区别？<br>Q55：提示工程、RAG、SFT、RL、RLHF应该分别在什么场景下应用？例如：快速迭代基本能力（提示工程）、用户个性化记忆（提示工程）、案例库和事实知识(RAG)、输出格式和语言风格(SFT)、领域深度思考能力和工具调用能力(RL)、根据用户反馈持续优化(RLHF)。</p>
<h2 id="附录：图解DeepSeek-R1（建议补充阅读DeepSeek的原始论文）"><a href="#附录：图解DeepSeek-R1（建议补充阅读DeepSeek的原始论文）" class="headerlink" title="附录：图解DeepSeek-R1（建议补充阅读DeepSeek的原始论文）"></a>附录：图解DeepSeek-R1（建议补充阅读DeepSeek的原始论文）</h2><p>Q56：DeepSeek-R1（简称R1）与DeepSeek-R1-Zero（简称R1-Zero）的训练过程有什么区别，各自有什么优缺点？既然R1-Zero生成的推理过程可读性差，在非推理任务上的表现也不如R1，那么R1-Zero存在的价值是什么？R1训练过程是如何解决R1-Zero的上述问题的？<br>Q57：DeepSeek是如何把R1的推理能力蒸馏到较小的模型中的？如果我们要自己蒸馏一个较小的垂直领域模型，如何尽可能保留R1在特定领域的能力？<br>Q58：R1-Zero的方法主要适用于有明确验证机制的任务（如数学、编程），如何将这一方法扩展到更主观的领域（如创意写作或战略分析）？<br>Q59：如果要在一个非推理型模型的基础上通过强化学习(RL)训练出一个1000以内的整数四则运算错误率低于1%的模型，预计基座模型至少需要多大？RL过程需要多少张GPU和多少训练时长？（提示：TinyZero）<br>Q60：在QwQ-32B推理模型的基础上，通过RL在类似OpenAI Deep Research的场景中强化垂直领域能力，如何构建训练数据集？预计需要多少张GPU和多少训练时长？</p>
</div><div class="post-end"><div class="post-prev"><a href="/2025/07/09/2025-7-9/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2025/07/04/2025-7-4/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E8%AF%9560%E9%97%AE"><span class="toc-content-number">1.</span> <span class="toc-content-text">大模型面试60问</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC1%E7%AB%A0-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B"><span class="toc-content-number">1.1.</span> <span class="toc-content-text">第1章　大语言模型简介</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC2%E7%AB%A0-%E8%AF%8D%E5%85%83%E5%92%8C%E5%B5%8C%E5%85%A5"><span class="toc-content-number">1.2.</span> <span class="toc-content-text">第2章　词元和嵌入</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC3%E7%AB%A0-LLM%E7%9A%84%E5%86%85%E9%83%A8%E6%9C%BA%E5%88%B6"><span class="toc-content-number">1.3.</span> <span class="toc-content-text">第3章　LLM的内部机制</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC4%E7%AB%A0-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-content-number">1.4.</span> <span class="toc-content-text">第4章　文本分类</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC5%E7%AB%A0-%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%E5%92%8C%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1"><span class="toc-content-number">1.5.</span> <span class="toc-content-text">第5章　文本聚类和主题建模</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC6%E7%AB%A0-%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B"><span class="toc-content-number">1.6.</span> <span class="toc-content-text">第6章　提示工程</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC7%E7%AB%A0-%E9%AB%98%E7%BA%A7%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%B7%A5%E5%85%B7"><span class="toc-content-number">1.7.</span> <span class="toc-content-text">第7章　高级文本生成技术与工具</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC8%E7%AB%A0-%E8%AF%AD%E4%B9%89%E6%90%9C%E7%B4%A2%E4%B8%8ERAG"><span class="toc-content-number">1.8.</span> <span class="toc-content-text">第8章　语义搜索与RAG</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC9%E7%AB%A0-%E5%A4%9A%E6%A8%A1%E6%80%81LLM"><span class="toc-content-number">1.9.</span> <span class="toc-content-text">第9章　多模态LLM</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC10%E7%AB%A0-%E6%9E%84%E5%BB%BA%E6%96%87%E6%9C%AC%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B"><span class="toc-content-number">1.10.</span> <span class="toc-content-text">第10章　构建文本嵌入模型</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC11%E7%AB%A0-%E4%B8%BA%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B"><span class="toc-content-number">1.11.</span> <span class="toc-content-text">第11章　为分类任务微调表示模型</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E7%AC%AC12%E7%AB%A0-%E5%BE%AE%E8%B0%83%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="toc-content-number">1.12.</span> <span class="toc-content-text">第12章　微调生成模型</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A%E5%9B%BE%E8%A7%A3DeepSeek-R1%EF%BC%88%E5%BB%BA%E8%AE%AE%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BBDeepSeek%E7%9A%84%E5%8E%9F%E5%A7%8B%E8%AE%BA%E6%96%87%EF%BC%89"><span class="toc-content-number">1.13.</span> <span class="toc-content-text">附录：图解DeepSeek-R1（建议补充阅读DeepSeek的原始论文）</span></a></li></ol></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2025 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>