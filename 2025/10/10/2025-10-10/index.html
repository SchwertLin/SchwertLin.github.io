<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="scaling law？扩展数据集是否值得？大模型的Scaling Law是OpenAI在2020年提出的概念[1]，具体如下:  对于Decoder-only的模型，计算量$C$(Flops), 模型参数量$N$, 数据大小$D$(token数)，三者满足: $C\approx 6ND$。(推导见本文最后) 模型的最终性能主要与计算量，模型参数量和数据大小三者相关，而与模型的具体结构(层数&#x2F;深度">
<meta property="og:type" content="article">
<meta property="og:title" content="Fri Oct 10 2025 00:00:00 GMT+0800 (中國標準時間)">
<meta property="og:url" content="http://example.com/2025/10/10/2025-10-10/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="scaling law？扩展数据集是否值得？大模型的Scaling Law是OpenAI在2020年提出的概念[1]，具体如下:  对于Decoder-only的模型，计算量$C$(Flops), 模型参数量$N$, 数据大小$D$(token数)，三者满足: $C\approx 6ND$。(推导见本文最后) 模型的最终性能主要与计算量，模型参数量和数据大小三者相关，而与模型的具体结构(层数&#x2F;深度">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/SchwertLin/Pic/main/img735bea31-48ce-4f4b-9eaa-f5f0d92a3c39.png">
<meta property="og:image" content="https://raw.githubusercontent.com/SchwertLin/Pic/main/imgd68d0dd8-cb89-40ff-97fc-63ff11a8a3f5.png">
<meta property="article:published_time" content="2025-10-10T07:03:46.000Z">
<meta property="article:modified_time" content="2025-10-11T06:17:52.641Z">
<meta property="article:author" content="Schwertlilien">
<meta property="article:tag" content="食品计算">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/SchwertLin/Pic/main/img735bea31-48ce-4f4b-9eaa-f5f0d92a3c39.png"><title>Fri Oct 10 2025 00:00:00 GMT+0800 (中國標準時間) - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>Fri Oct 10 2025 00:00:00 GMT+0800 (中國標準時間)</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2025-10-10</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2025-10-11</time></div>
<div class="post-categories"><i class="icon fa-regular fa-folder-open" title="分类"></i><a class="post-category" href="/categories/%E5%B7%A5%E4%BD%9C/">工作</a></div>
<div class="post-tags"><i class="icon fa-solid fa-tags" title="标签"></i><a class="post-tag" href="/tags/%E9%A3%9F%E5%93%81%E8%AE%A1%E7%AE%97/">食品计算</a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约1.3K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><h1 id="scaling-law？扩展数据集是否值得？"><a href="#scaling-law？扩展数据集是否值得？" class="headerlink" title="scaling law？扩展数据集是否值得？"></a>scaling law？扩展数据集是否值得？</h1><p>大模型的Scaling Law是OpenAI在2020年提出的概念[1]，具体如下:</p>
<ol>
<li>对于Decoder-only的模型，计算量$C$(Flops), 模型参数量$N$, 数据大小$D$(token数)，三者满足: $C\approx 6ND$。(推导见本文最后)</li>
<li>模型的最终性能<strong>主要与</strong>计算量，模型参数量和数据大小三者相关，而与模型的具体结构(层数/深度/宽度)基本无关。</li>
</ol>
<blockquote>
<p>这导致固定模型的总参数量$N$，调整层数/深度/宽度，不同模型的性能差距很小，大部分在2%以内。</p>
<p>当非嵌入参数总数$N$固定时，损失（性能指标）在 “广泛的模型形状范围” 内仅变化几个百分点。参数计数的微小差异，可通过 “以 (L(N)) 为基线的拟合” 来补偿；模型结构的 “纵横比”（如层数与模型维度的比例）甚至能在 40 倍范围内变化，对性能影响仍很小。</p>
<ul>
<li><strong>左图（Feed-Forward Ratio）</strong>：横轴是 “前馈网络比例（(d_{\text{ff}} / d_{\text{model}})）”，纵轴是 “损失增加量”。不同 (d_{\text{model}}) 配置下（蓝色$n_{head}=8$,黄色$d_{model}/n_{head}=64$），损失随前馈比例的变化幅度都很小（当横轴从(10^0)变化到(10^1)时，纵轴上升幅度都很小），说明前馈比例对性能影响弱。</li>
<li><strong>中图（Aspect Ratio）</strong>：横轴是 “纵横比（(d_{\text{model}} / n_{\text{layer}})）”，不同参数规模（50M、274M、1.5B）的模型，在广泛的纵横比范围内，损失（性能）都很接近。即 “纵横比变化时，只要参数总数固定，性能差异小”。</li>
<li><strong>右图（Attention Head Dimension）</strong>：横轴是 “注意力头维度（(d_{\text{model}} / d_{\text{head}})）”，纵轴是损失。不同头维度（256、512、1024）下，损失变化幅度小；且注释提到 “22% 的额外计算，就能补偿 1% 的损失增加”，进一步说明结构变化对性能的影响可通过计算量微调来弥补。</li>
</ul>
</blockquote>
<p><img src="https://raw.githubusercontent.com/SchwertLin/Pic/main/img735bea31-48ce-4f4b-9eaa-f5f0d92a3c39.png" alt="735bea31-48ce-4f4b-9eaa-f5f0d92a3c39"></p>
<ol>
<li>对于计算量$C$，模型参数量$N$和数据大小$D$，当不受其他两个因素制约是，模型性能与每个因素都呈现幂律关系。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/SchwertLin/Pic/main/imgd68d0dd8-cb89-40ff-97fc-63ff11a8a3f5.png" alt="语言建模性能会随着模型大小（参数）、数据集大小、训练计算量的增加而平滑提升。"></p>
<h1 id="一些启示"><a href="#一些启示" class="headerlink" title="一些启示"></a>一些启示</h1><ol>
<li><strong>数据规模的 “边际收益递减”</strong>当数据从 10k 扩展到 100k 时，性能提升幅度会小于从 1k 到 10k 的提升，但<strong>若当前数据仅 10k，仍处于 “收益较显著的区间”</strong>（尤其是小数据集阶段）。<ul>
<li>例：某语义分割任务中，数据从 10k→50k 时，IoU 可能从 70→80（提升 10 个点）；但从 50k→100k 时，IoU 可能仅从 80→85（提升 5 个点）。</li>
</ul>
</li>
<li><strong>“数据 + 模型 + 训练时长” 必须协同缩放</strong><ul>
<li><strong>模型容量</strong>：只有大模型（如 SwinV2-G、SAM 等）才能 “消化” 大规模数据。若你用的是轻量级模型（如小型 U-Net），即使数据扩到 100k，性能也可能因模型容量不足而<strong>饱和甚至过拟合</strong>。</li>
<li><strong>训练时长</strong>：大数据集需要更长的训练迭代（如从 125k 步→500k 步）才能发挥价值。若训练不充分，100k 数据的性能提升可能不明显。</li>
</ul>
</li>
<li><strong>数据多样性比 “单纯数量” 更重要</strong>分割标注成本极高（像素级标注），若 100k 图片的<strong>场景、目标分布单一</strong>（如仅室内场景、仅小目标），模型泛化能力提升有限。需通过以下方式增强多样性：<ul>
<li><strong>生成式数据增强</strong>：用 GAN、扩散模型生成合成标注数据（如 DiverGen 方法，通过 “类别多样性 + prompt 多样性 + 生成模型多样性” 扩充分布）；</li>
<li><strong>跨域数据融合</strong>：引入公开数据集（如 Cityscapes、ADE20K）或合成数据集（如 SYNTHIA）补充场景。</li>
</ul>
</li>
</ol>
<h1 id="具体做法"><a href="#具体做法" class="headerlink" title="具体做法"></a>具体做法</h1><h4 id="步骤-1：先做-“小步扩展实验”，验证数据规模的影响"><a href="#步骤-1：先做-“小步扩展实验”，验证数据规模的影响" class="headerlink" title="步骤 1：先做 “小步扩展实验”，验证数据规模的影响"></a>步骤 1：先做 “小步扩展实验”，验证数据规模的影响</h4><ul>
<li><strong>实验设计</strong>：先将数据扩到 20k、50k，保持模型和训练策略不变，测试验证集性能（IoU、AP 等）。<ul>
<li>若 50k 时验证损失（或 IoU）仍在<strong>持续下降</strong>，说明数据未饱和，扩到 100k 可能有收益；</li>
<li>若 50k 时性能已<strong>趋于平稳</strong>，说明当前模型 / 训练策略下数据已饱和，需先优化模型或增强数据多样性。</li>
</ul>
</li>
</ul>
<h4 id="步骤-2：拟合-“数据规模-性能”-的幂律曲线，定量预测收益"><a href="#步骤-2：拟合-“数据规模-性能”-的幂律曲线，定量预测收益" class="headerlink" title="步骤 2：拟合 “数据规模 - 性能” 的幂律曲线，定量预测收益"></a>步骤 2：拟合 “数据规模 - 性能” 的幂律曲线，定量预测收益</h4><ul>
<li><strong>幂律公式</strong>：分割任务中，性能（如 IoU）与数据量 N 通常满足 (\text{IoU} = a \cdot N^b)（b 为负数，绝对值越小表示收益递减越慢）。</li>
<li><strong>操作</strong>：收集不同数据规模（如 1k、5k、10k、20k）下的验证集 IoU，用对数回归拟合 (\log(\text{IoU}) = \log(a) + b \cdot \log(N))，预测 100k 时的 IoU。</li>
</ul>
<h4 id="步骤-3：评估-“成本-收益比”"><a href="#步骤-3：评估-“成本-收益比”" class="headerlink" title="步骤 3：评估 “成本 - 收益比”"></a>步骤 3：评估 “成本 - 收益比”</h4><ul>
<li><strong>成本</strong>：标注 100k 图片的人力、时间、算力投入（尤其是像素级标注的成本很高）；</li>
<li><strong>收益</strong>：性能提升对业务的价值（如 IoU 从 75→85 是否能满足下游应用需求）。</li>
</ul>
</div><div class="post-end"><div class="post-prev"><a href="/2025/10/11/2025-10-11/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2025/10/09/2025-10-9/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#scaling-law%EF%BC%9F%E6%89%A9%E5%B1%95%E6%95%B0%E6%8D%AE%E9%9B%86%E6%98%AF%E5%90%A6%E5%80%BC%E5%BE%97%EF%BC%9F"><span class="toc-content-number">1.</span> <span class="toc-content-text">scaling law？扩展数据集是否值得？</span></a></li><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#%E4%B8%80%E4%BA%9B%E5%90%AF%E7%A4%BA"><span class="toc-content-number">2.</span> <span class="toc-content-text">一些启示</span></a></li><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#%E5%85%B7%E4%BD%93%E5%81%9A%E6%B3%95"><span class="toc-content-number">3.</span> <span class="toc-content-text">具体做法</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E6%AD%A5%E9%AA%A4-1%EF%BC%9A%E5%85%88%E5%81%9A-%E2%80%9C%E5%B0%8F%E6%AD%A5%E6%89%A9%E5%B1%95%E5%AE%9E%E9%AA%8C%E2%80%9D%EF%BC%8C%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-content-number">3.0.0.1.</span> <span class="toc-content-text">步骤 1：先做 “小步扩展实验”，验证数据规模的影响</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E6%AD%A5%E9%AA%A4-2%EF%BC%9A%E6%8B%9F%E5%90%88-%E2%80%9C%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1-%E6%80%A7%E8%83%BD%E2%80%9D-%E7%9A%84%E5%B9%82%E5%BE%8B%E6%9B%B2%E7%BA%BF%EF%BC%8C%E5%AE%9A%E9%87%8F%E9%A2%84%E6%B5%8B%E6%94%B6%E7%9B%8A"><span class="toc-content-number">3.0.0.2.</span> <span class="toc-content-text">步骤 2：拟合 “数据规模 - 性能” 的幂律曲线，定量预测收益</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E6%AD%A5%E9%AA%A4-3%EF%BC%9A%E8%AF%84%E4%BC%B0-%E2%80%9C%E6%88%90%E6%9C%AC-%E6%94%B6%E7%9B%8A%E6%AF%94%E2%80%9D"><span class="toc-content-number">3.0.0.3.</span> <span class="toc-content-text">步骤 3：评估 “成本 - 收益比”</span></a></li></ol></li></ol></li></ol></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2026 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>