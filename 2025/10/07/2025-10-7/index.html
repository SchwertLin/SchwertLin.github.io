<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="在具体的代码层面如何实现仅更新CLIP中的部分参数？ CLIP-V 局部参数微调与层冻结的代码实施在代码层面，“局部参数微调 + 部分层冻结” 的核心是 “通过参数的‘requires_grad’属性控制是否更新”，不同 CLIP 架构（CNN&#x2F;ViT）的实施方式略有差异，以 PyTorch 框架为例，具体步骤如下： 1. 核心逻辑：区分 “冻结层” 与 “微调层” 的参数梯度PyTorch 中，">
<meta property="og:type" content="article">
<meta property="og:title" content="2025-10-7">
<meta property="og:url" content="http://example.com/2025/10/07/2025-10-7/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="在具体的代码层面如何实现仅更新CLIP中的部分参数？ CLIP-V 局部参数微调与层冻结的代码实施在代码层面，“局部参数微调 + 部分层冻结” 的核心是 “通过参数的‘requires_grad’属性控制是否更新”，不同 CLIP 架构（CNN&#x2F;ViT）的实施方式略有差异，以 PyTorch 框架为例，具体步骤如下： 1. 核心逻辑：区分 “冻结层” 与 “微调层” 的参数梯度PyTorch 中，">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-07T10:18:54.000Z">
<meta property="article:modified_time" content="2025-10-09T07:27:38.689Z">
<meta property="article:author" content="Schwertlilien">
<meta name="twitter:card" content="summary"><title>2025-10-7 - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>2025-10-7</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2025-10-07</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2025-10-09</time></div>

</div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约0.9K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><p>在具体的代码层面如何实现仅更新CLIP中的部分参数？</p>
<h2 id="CLIP-V-局部参数微调与层冻结的代码实施"><a href="#CLIP-V-局部参数微调与层冻结的代码实施" class="headerlink" title="CLIP-V 局部参数微调与层冻结的代码实施"></a>CLIP-V 局部参数微调与层冻结的代码实施</h2><p>在代码层面，“局部参数微调 + 部分层冻结” 的核心是 <strong>“通过参数的‘requires_grad’属性控制是否更新”</strong>，不同 CLIP 架构（CNN/ViT）的实施方式略有差异，以 PyTorch 框架为例，具体步骤如下：</p>
<h3 id="1-核心逻辑：区分-“冻结层”-与-“微调层”-的参数梯度"><a href="#1-核心逻辑：区分-“冻结层”-与-“微调层”-的参数梯度" class="headerlink" title="1. 核心逻辑：区分 “冻结层” 与 “微调层” 的参数梯度"></a>1. 核心逻辑：区分 “冻结层” 与 “微调层” 的参数梯度</h3><p>PyTorch 中，模型参数的<code>requires_grad</code>属性决定了该参数是否会在反向传播中更新：</p>
<ul>
<li><code>requires_grad=True</code>：参数会被优化（微调）；</li>
<li><code>requires_grad=False</code>：参数固定（冻结），反向传播时不计算梯度。</li>
</ul>
<p>实施的关键是 “先加载预训练 CLIP 模型，再针对性修改特定层的<code>requires_grad</code>”，而非从头定义模型。</p>
<h3 id="2-分架构代码示例（以-Hugging-Face-transformers库的CLIPVisionModel为例）"><a href="#2-分架构代码示例（以-Hugging-Face-transformers库的CLIPVisionModel为例）" class="headerlink" title="2. 分架构代码示例（以 Hugging Face transformers库的CLIPVisionModel为例）"></a>2. 分架构代码示例（以 Hugging Face <code>transformers</code>库的<code>CLIPVisionModel</code>为例）</h3><h4 id="（1）CNN-类-CLIP（如-ConvNeXt-B-CLIP）：微调全部层-低学习率"><a href="#（1）CNN-类-CLIP（如-ConvNeXt-B-CLIP）：微调全部层-低学习率" class="headerlink" title="（1）CNN 类 CLIP（如 ConvNeXt-B/CLIP）：微调全部层 + 低学习率"></a>（1）CNN 类 CLIP（如 ConvNeXt-B/CLIP）：微调全部层 + 低学习率</h4><p>CNN 类 CLIP（如 ConvNeXt、ResNet）的预训练特征提取能力强，微调全部层但需降低学习率（避免破坏预训练权重），代码逻辑如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPVisionModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载预训练CLIP视觉编码器（ConvNeXt-B/CLIP）</span></span><br><span class="line">clip_v = CLIPVisionModel.from_pretrained(<span class="string">&quot;openai/clip-vit-base-patch32&quot;</span>)  <span class="comment"># 示例，实际需对应ConvNeXt版本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 微调全部层：所有参数requires_grad=True（默认加载后为True，无需额外操作）</span></span><br><span class="line"><span class="comment"># 3. 训练时单独设置CLIP-V的学习率为其他模块的1/100（如PSM模块学习率1e-3，CLIP-V为1e-5）</span></span><br><span class="line">optimizer = torch.optim.AdamW([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: clip_v.parameters(), <span class="string">&quot;lr&quot;</span>: <span class="number">1e-5</span>&#125;,  <span class="comment"># CLIP-V低学习率微调</span></span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: psm_module.parameters(), <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>&#125;  <span class="comment"># PSM模块正常学习率</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="（2）ViT-类-CLIP（如-ViT-B-16）：仅微调位置编码与注意力投影层"><a href="#（2）ViT-类-CLIP（如-ViT-B-16）：仅微调位置编码与注意力投影层" class="headerlink" title="（2）ViT 类 CLIP（如 ViT-B/16）：仅微调位置编码与注意力投影层"></a>（2）ViT 类 CLIP（如 ViT-B/16）：仅微调位置编码与注意力投影层</h4><p>ViT 的 Transformer 块（<code>encoder.layers</code>）是核心语义提取模块，需冻结；仅微调 “位置编码（<code>embeddings.position_embeddings</code>）” 和 “注意力输入投影层（<code>encoder.layers.*.self_attn.q_proj</code>等）”，代码逻辑如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPVisionModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载预训练ViT类CLIP视觉编码器</span></span><br><span class="line">clip_v = CLIPVisionModel.from_pretrained(<span class="string">&quot;openai/clip-vit-base-patch32&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 冻结所有层（先全局设为False）</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> clip_v.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 解冻“位置编码层”（微调以适配掩码区域的空间信息）</span></span><br><span class="line">clip_v.embeddings.position_embeddings.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 解冻“注意力输入投影层”（所有Transformer层的q_proj、k_proj、v_proj）</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> clip_v.encoder.layers:</span><br><span class="line">    layer.self_attn.q_proj.requires_grad = <span class="literal">True</span></span><br><span class="line">    layer.self_attn.k_proj.requires_grad = <span class="literal">True</span></span><br><span class="line">    layer.self_attn.v_proj.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 训练时设置解冻层的学习率（如1e-4，高于CNN类CLIP）</span></span><br><span class="line">optimizer = torch.optim.AdamW([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: clip_v.embeddings.position_embeddings.parameters(), <span class="string">&quot;lr&quot;</span>: <span class="number">1e-4</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> layer <span class="keyword">in</span> clip_v.encoder.layers <span class="keyword">for</span> p <span class="keyword">in</span> [layer.self_attn.q_proj.parameters(), </span><br><span class="line">                                                               layer.self_attn.k_proj.parameters(), </span><br><span class="line">                                                               layer.self_attn.v_proj.parameters()]], <span class="string">&quot;lr&quot;</span>: <span class="number">1e-4</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: psm_module.parameters(), <span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h3 id="3-工程验证：确保冻结层无梯度更新"><a href="#3-工程验证：确保冻结层无梯度更新" class="headerlink" title="3. 工程验证：确保冻结层无梯度更新"></a>3. 工程验证：确保冻结层无梯度更新</h3><p>训练中可通过<code>torch.nn.utils.clip_grad_norm_</code>或打印梯度值，验证冻结层是否无梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播后，检查冻结层（如ViT的encoder.layers.0.layer_norm1）的梯度</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> clip_v.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;encoder.layers.0.layer_norm1&quot;</span> <span class="keyword">in</span> name:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> grad: <span class="subst">&#123;param.grad <span class="keyword">is</span> <span class="literal">None</span>&#125;</span>&quot;</span>)  <span class="comment"># 输出True，说明无梯度（冻结成功）</span></span><br></pre></td></tr></table></figure>
</div><div class="post-end"><div class="post-prev"><a href="/2025/10/09/2025-10-9/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2025/10/06/2025-10-6/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#CLIP-V-%E5%B1%80%E9%83%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%E4%B8%8E%E5%B1%82%E5%86%BB%E7%BB%93%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%96%BD"><span class="toc-content-number">1.</span> <span class="toc-content-text">CLIP-V 局部参数微调与层冻结的代码实施</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#1-%E6%A0%B8%E5%BF%83%E9%80%BB%E8%BE%91%EF%BC%9A%E5%8C%BA%E5%88%86-%E2%80%9C%E5%86%BB%E7%BB%93%E5%B1%82%E2%80%9D-%E4%B8%8E-%E2%80%9C%E5%BE%AE%E8%B0%83%E5%B1%82%E2%80%9D-%E7%9A%84%E5%8F%82%E6%95%B0%E6%A2%AF%E5%BA%A6"><span class="toc-content-number">1.1.</span> <span class="toc-content-text">1. 核心逻辑：区分 “冻结层” 与 “微调层” 的参数梯度</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#2-%E5%88%86%E6%9E%B6%E6%9E%84%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B%EF%BC%88%E4%BB%A5-Hugging-Face-transformers%E5%BA%93%E7%9A%84CLIPVisionModel%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="toc-content-number">1.2.</span> <span class="toc-content-text">2. 分架构代码示例（以 Hugging Face transformers库的CLIPVisionModel为例）</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%EF%BC%881%EF%BC%89CNN-%E7%B1%BB-CLIP%EF%BC%88%E5%A6%82-ConvNeXt-B-CLIP%EF%BC%89%EF%BC%9A%E5%BE%AE%E8%B0%83%E5%85%A8%E9%83%A8%E5%B1%82-%E4%BD%8E%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-content-number">1.2.1.</span> <span class="toc-content-text">（1）CNN 类 CLIP（如 ConvNeXt-B&#x2F;CLIP）：微调全部层 + 低学习率</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%EF%BC%882%EF%BC%89ViT-%E7%B1%BB-CLIP%EF%BC%88%E5%A6%82-ViT-B-16%EF%BC%89%EF%BC%9A%E4%BB%85%E5%BE%AE%E8%B0%83%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8A%95%E5%BD%B1%E5%B1%82"><span class="toc-content-number">1.2.2.</span> <span class="toc-content-text">（2）ViT 类 CLIP（如 ViT-B&#x2F;16）：仅微调位置编码与注意力投影层</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#3-%E5%B7%A5%E7%A8%8B%E9%AA%8C%E8%AF%81%EF%BC%9A%E7%A1%AE%E4%BF%9D%E5%86%BB%E7%BB%93%E5%B1%82%E6%97%A0%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0"><span class="toc-content-number">1.3.</span> <span class="toc-content-text">3. 工程验证：确保冻结层无梯度更新</span></a></li></ol></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2026 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>