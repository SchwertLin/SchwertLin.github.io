<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="无实意吐槽：这篇文章也太6了。基本无差评，希望我也能写这种。 https:&#x2F;&#x2F;openreview.net&#x2F;forum?id&#x3D;Ha6RTeWMd0  Psychically Informed 3D Food Reconstruction:  Methods and Results 准确估算食物份量是营养分析和膳食评估中的关键挑战。近期的三维重建方法主要侧重于表面几何，往往忽略了精确估算份量所需的">
<meta property="og:type" content="article">
<meta property="og:title" content="2025-10-9">
<meta property="og:url" content="http://example.com/2025/10/09/2025-10-9/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="无实意吐槽：这篇文章也太6了。基本无差评，希望我也能写这种。 https:&#x2F;&#x2F;openreview.net&#x2F;forum?id&#x3D;Ha6RTeWMd0  Psychically Informed 3D Food Reconstruction:  Methods and Results 准确估算食物份量是营养分析和膳食评估中的关键挑战。近期的三维重建方法主要侧重于表面几何，往往忽略了精确估算份量所需的">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-09T07:26:49.000Z">
<meta property="article:modified_time" content="2025-10-10T09:27:34.701Z">
<meta property="article:author" content="Schwertlilien">
<meta name="twitter:card" content="summary"><title>2025-10-9 - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>2025-10-9</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2025-10-09</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2025-10-10</time></div>

</div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约5.1K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><blockquote>
<p>无实意吐槽：这篇文章也太6了。基本无差评，希望我也能写这种。</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=Ha6RTeWMd0">https://openreview.net/forum?id=Ha6RTeWMd0</a></p>
</blockquote>
<h1 id="Psychically-Informed-3D-Food-Reconstruction-Methods-and-Results"><a href="#Psychically-Informed-3D-Food-Reconstruction-Methods-and-Results" class="headerlink" title="Psychically Informed 3D Food Reconstruction:  Methods and Results"></a>Psychically Informed 3D Food Reconstruction:  Methods and Results</h1><blockquote>
<p>准确估算食物份量是营养分析和膳食评估中的关键挑战。近期的三维重建方法主要侧重于表面几何，往往忽略了精确估算份量所需的体积精度。本文提出了三种基于有限二维输入进行真实尺寸三维食物重建和体积估算的方法。这些方法利用可见棋盘格等物理参考，能够在不同条件下从单视图或多视图图像进行精确的度量建模。我们评估了每种方法，并使用了一系列纹理、形状和相机姿态各异的食品样本，并评估了它们在体积估算和几何精度方面的表现。结果证明了这些可解释流程在实现稳健的三维重建和精确份量估算方面的有效性。这些贡献推动了实用且可扩展的膳食监测工具的开发，并有望应用于个人健康管理和临床营养追踪。</p>
</blockquote>
<p>论文的主题是：用“有限2D图像+物理参考”重建真实比例的3D食品模型，精准估算食品体积，最终解决「膳食评估（Dietary Assessment）」里的“份量不准”难题。</p>
<p>这个更像是一篇综述？不太好定义此paper到底是在做什么工作。主要是因为摘要我就觉得写的不好，关于具体的方法描写、没有；对于实验的结果数据、没有；感觉比较空。没有具体的细节。</p>
<h2 id="背景：之前的3D重建方法"><a href="#背景：之前的3D重建方法" class="headerlink" title="背景：之前的3D重建方法"></a>背景：之前的3D重建方法</h2><p>论文把之前的方案归为4类：主要是之前的3D重建方面，没有人做有关于food的方案。所以导致现有的方案直接迁移到food上面就会出现各种各样的问题。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>传统方法类型</th>
<th>核心思路</th>
<th>致命问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>立体视觉法（Stereo）</td>
<td>多视角图像拼3D结构</td>
<td>要拍多张图，普通人拍美食不会特意换角度</td>
</tr>
<tr>
<td>模型基法（Model-based）</td>
<td>用预设模板（比如“圆形蛋糕”）套</td>
<td>食品形状千差万别（比如不规则的牛排），模板套不上</td>
</tr>
<tr>
<td>深度相机法</td>
<td>靠深度相机拍距离信息</td>
<td>要特殊硬件（普通手机没有），不接地气</td>
</tr>
<tr>
<td>深度学习法</td>
<td>用数据训模型直接估体积</td>
<td>缺“可解释性”（算错了不知道为啥），换个食品就不准</td>
</tr>
</tbody>
</table>
</div>
<p>此外，还有个共性问题：<strong>没有“物理参考”</strong>。</p>
<blockquote>
<p>比如你拍一张汉堡照片，不知道照片里“1厘米”对应真实世界多少，重建的3D模型是“无单位的”，没法算实际体积——这也是论文里反复强调“物理参考（比如棋盘格）”的原因：棋盘格每格大小已知（1.2cm），相当于给模型定了“尺子”，能把3D模型缩放到真实比例。</p>
</blockquote>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>论文没自己造新数据集，而是<strong>用了MetaFood3D的子集</strong>，模拟普通人用手机拍美食的场景：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>难度等级</th>
<th>类别数量</th>
<th>具体食品类别</th>
<th>每类帧数</th>
<th>核心测试目标</th>
</tr>
</thead>
<tbody>
<tr>
<td>Easy（200张图）</td>
<td>8</td>
<td>草莓、肉桂卷、猪排、玉米、法式吐司、三明治、汉堡、蛋糕</td>
<td>199-200</td>
<td>多视角下的高精度重建</td>
</tr>
<tr>
<td>Medium（30张图）</td>
<td>7</td>
<td>蓝莓松饼、香蕉、三文鱼、牛排、墨西哥卷饼、热狗、鸡肉块</td>
<td>30</td>
<td>中等视角数量下的适应性</td>
</tr>
<tr>
<td>Hard（1张图）</td>
<td>5</td>
<td>全麦贝果、可颂、虾、华夫饼、披萨</td>
<td>1</td>
<td>单视角下的几何推断与尺度校准</td>
</tr>
</tbody>
</table>
</div>
<p>该子集的核心特点是：<strong>每个食品都有 “3D 扫描真值网格”（用于体积 / 形状误差计算）、“RGB-D 视频 / 图像”（输入数据）、“棋盘格物理参考”（尺度校准）</strong>，</p>
<p>真值保证：每个食品都用3D扫描仪（Revopoint POP2）扫了“真实3D模型”（体积、形状都是准的），还有棋盘格作为物理参考，确保缩放不跑偏。</p>
<p>==我认为测试的数据量比较少。不足以说明有效性。==总图片数计算：共 1814 张按 “难度等级” 拆分统计（对应 Table I 中 “Number of Frames” 列）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>难度等级</th>
<th>类别范围</th>
<th>类别数量</th>
<th>单类帧数</th>
<th>该等级总帧数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Easy</td>
<td>1-8（草莓、肉桂卷、排骨等）</td>
<td>8</td>
<td>199（仅类别 1）、200（类别 2-8）</td>
<td>199 + 200×7 = 1599</td>
</tr>
<tr>
<td>Medium</td>
<td>9-15（蓝莓松饼、香蕉、三文鱼等）</td>
<td>7</td>
<td>30（所有类别）</td>
<td>7×30 = 210</td>
</tr>
<tr>
<td>Hard</td>
<td>16-20（贝果、可颂、虾等）</td>
<td>5</td>
<td>1（所有类别）</td>
<td>5×1 = 5</td>
</tr>
<tr>
<td><strong>合计</strong></td>
<td>1-20</td>
<td>20</td>
<td>-</td>
<td>1599 + 210 + 5 = <strong>1814 张</strong></td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><strong>类别覆盖不足</strong>：未说明子集筛选的科学标准（如是否覆盖典型形状/纹理的食品、是否补充标注缺失信息），仅简单按“难度分级”划分样本，且未覆盖高难度食品类型（如汤汁类、堆叠类），无法验证方法的泛化能力；</li>
<li><strong>样本量严重匮乏</strong>：未通过子集扩展、标注增强、模态补充等方式产生新数据资源，只有1.8k张，一共20类食物。</li>
</ol>
<h2 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h2><p>方法是<strong>三个从MetaFood Workshop挑战赛里选出的top方法</strong>，围绕“有限2D输入+物理参考=真实3D重建”展开。</p>
<h3 id="1-VolETA：“全能冠军”，体积估算最准"><a href="#1-VolETA：“全能冠军”，体积估算最准" class="headerlink" title="1. VolETA：“全能冠军”，体积估算最准"></a>1. VolETA：“全能冠军”，体积估算最准</h3><blockquote>
<p>核心逻辑：多技术融合，把“精度”拉满</p>
</blockquote>
<p>流程可以拆成4步，像搭积木一样：</p>
<ol>
<li><strong>选关键帧</strong>：先过滤掉模糊、重复的图（比如拍视频时相邻的相似帧），只留34.8%的有效帧——减少计算量，还能避免噪声；</li>
<li><strong>估相机姿态+分参考物</strong>：用PixSfM（一种SfM方法）算相机拍每张图时的位置/角度；同时用SAM（分割模型）把棋盘格（物理参考）从图里抠出来，再用XMem++跟踪所有帧里的棋盘格，确保参考一致；</li>
<li><strong>建3D网格</strong>：把处理好的图和相机姿态喂给NeuS2（神经隐式表面模型），重建出食品和棋盘格的3D网格，再删掉网格里的小碎片（比如孤立的点）；</li>
<li><strong>算真实比例</strong>：先手动用MeshLab测棋盘格网格的大小，初定一个缩放因子S；再用深度图算食品的宽/长/高，微调S到Sf——确保3D模型和真实食品一样大，最后算体积。</li>
</ol>
<h4 id="亮点："><a href="#亮点：" class="headerlink" title="亮点："></a>亮点：</h4><ul>
<li>单视角场景也能扛：用One-2-3-45模型，从一张图就能生成3D网格，再复用之前算的缩放因子，精度不掉太多；</li>
<li>体积最准：最终MAPE（平均百分比误差）10.98%，比GPT-4o（带上下文的34.33%）低了24个百分点，而且标准差最小（10.104），说明稳。</li>
</ul>
<h3 id="2-ININ-VIAUN：“稳扎稳打”，靠后处理补短板"><a href="#2-ININ-VIAUN：“稳扎稳打”，靠后处理补短板" class="headerlink" title="2. ININ-VIAUN：“稳扎稳打”，靠后处理补短板"></a>2. ININ-VIAUN：“稳扎稳打”，靠后处理补短板</h3><blockquote>
<p>核心逻辑：多方案备选，用后处理修“瑕疵”</p>
</blockquote>
<p>思路更灵活，分“多视角”和“单视角”两套方案：</p>
<ul>
<li><strong>多视角（1-15号食品）</strong>：先用COLMAP算相机姿态，再试三种重建方法（COLMAP、DiffusioNeRF、NeRF2Mesh），选视觉和几何最准的那个；</li>
<li><strong>单视角（16-20号食品）</strong>：用ZeroNVS模型（擅长单图生成3D），但要优化相机参数（因为只有一张图，参数不准），再结合棋盘格和深度图定比例；</li>
<li><strong>关键后处理</strong>：重建的网格常有洞、噪声，用MeshFix补洞，拉普拉斯平滑去噪声——相当于给模型“修容”，让形状更规整。</li>
</ul>
<h4 id="亮点与不足："><a href="#亮点与不足：" class="headerlink" title="亮点与不足："></a>亮点与不足：</h4><ul>
<li>多视角表现还行，但单视角依赖深度信息和棋盘格，泛化稍弱；</li>
<li>整体MAPE16.30%，比VolETA高，标准差也大（23.726），稳定性一般。</li>
</ul>
<h3 id="3-FoodRiddle：“形状专家”，3D模型最像真实食品"><a href="#3-FoodRiddle：“形状专家”，3D模型最像真实食品" class="headerlink" title="3. FoodRiddle：“形状专家”，3D模型最像真实食品"></a>3. FoodRiddle：“形状专家”，3D模型最像真实食品</h3><blockquote>
<p>核心逻辑：重“视觉保真”，单视角重建最强</p>
</blockquote>
<p>主打“把形状做对”，流程分两类：</p>
<ul>
<li><strong>多视角</strong>：用带SuperPoint/SuperGlue的COLMAP算姿态（比普通COLMAP多提特征点，适合纹理少的食品，比如面包），再用2D高斯溅射重建网格，最后用泊松表面重建补全网格（比如食品底部看不见的部分）；</li>
<li><strong>单视角</strong>：先用生成模型（比如LGM、One-2-3-45）造几个“虚拟视角”，再用稀疏视角重建模型，缩放靠棋盘格校准——相当于“脑补”多角度，让形状更准。</li>
</ul>
<h4 id="亮点：-1"><a href="#亮点：-1" class="headerlink" title="亮点："></a>亮点：</h4><ul>
<li>形状最准：Chamfer距离（衡量3D模型与真值的相似度）最小，平均0.0031米，比VolETA（0.0073米）小一半多；</li>
<li>单视角体积最准：Hard难度下MAPE15.56%，比另外两个方法都低，适合真实场景。</li>
</ul>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>论文的实验设计核心看两个维度：<strong>体积准不准</strong>和<strong>形状像不像</strong>。</p>
<h3 id="1-体积精度（MAPE越低越好）"><a href="#1-体积精度（MAPE越低越好）" class="headerlink" title="1. 体积精度（MAPE越低越好）"></a>1. 体积精度（MAPE越低越好）</h3><div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>整体MAPE</th>
<th>多视角MAPE</th>
<th>单视角MAPE</th>
<th>标准差</th>
</tr>
</thead>
<tbody>
<tr>
<td>VolETA</td>
<td>10.98%</td>
<td>7.84%</td>
<td>31.89%</td>
<td>10.104</td>
</tr>
<tr>
<td>FoodRiddle</td>
<td>11.73%</td>
<td>8.XX%</td>
<td>15.56%</td>
<td>11.579</td>
</tr>
<tr>
<td>ININ-VIAUN</td>
<td>16.30%</td>
<td>12.XX%</td>
<td>42.92%</td>
<td>23.726</td>
</tr>
<tr>
<td>传统方法（如3D-Assisted）</td>
<td>41.57%</td>
<td>-</td>
<td>-</td>
<td>45.486</td>
</tr>
<tr>
<td>GPT-4o（带上下文）</td>
<td>34.33%</td>
<td>-</td>
<td>-</td>
<td>22.183</td>
</tr>
</tbody>
</table>
</div>
<p>结论：三个方法都碾压传统方法和GPT-4o；VolETA多视角最准，FoodRiddle单视角最准。</p>
<h3 id="2-形状精度（Chamfer距离越小越好）"><a href="#2-形状精度（Chamfer距离越小越好）" class="headerlink" title="2. 形状精度（Chamfer距离越小越好）"></a>2. 形状精度（Chamfer距离越小越好）</h3><div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>平均Chamfer距离（米）</th>
<th>总距离（米）</th>
</tr>
</thead>
<tbody>
<tr>
<td>FoodRiddle</td>
<td>0.0031</td>
<td>0.0556</td>
</tr>
<tr>
<td>ININ-VIAUN</td>
<td>0.0039</td>
<td>0.0694</td>
</tr>
<tr>
<td>VolETA</td>
<td>0.0073</td>
<td>0.1306</td>
</tr>
</tbody>
</table>
</div>
<p>结论：FoodRiddle在形状上断层领先，适合对“视觉还原”有要求的场景（比如食品教学、AR展示）。</p>
<h2 id="一些讨论"><a href="#一些讨论" class="headerlink" title="一些讨论"></a>一些讨论</h2><p>说是有助于后续的膳食评估，但是也没进行这一部分的扩展研究。未继承MetaFood3D的营养标注、重量数据，仅依赖3D扫描体积作为真值，缺失膳食评估场景的核心标注（如热量、宏量营养素），降低数据价值。</p>
<p>论文指出了三个方法的共性和个性局限：</p>
<ol>
<li><p><strong>共性问题</strong>：</p>
<ul>
<li>有手动步骤：VolETA要给分割提示，ININ-VIAUN要选最优重建结果，FoodRiddle要调缩放因子——自动化不够，普通人用不了；</li>
<li>没测复杂场景：比如背景乱（比如餐桌上有很多杂物）、食品复杂（比如带汤汁的面条）、拍摄条件差（逆光、远拍）；</li>
<li>输入要求高：需要深度图、食品掩码，普通手机拍的图没有这些，得额外处理。</li>
</ul>
</li>
<li><p><strong>个性问题</strong>：</p>
<ul>
<li>VolETA：单视角流程复杂（用One-2-3-45），可以简化；</li>
<li>ININ-VIAUN：太依赖后处理（补洞、平滑），说明初始重建质量一般；</li>
<li>FoodRiddle：单视角假设“食品和棋盘格在同一平面”，如果食品垫了盘子（比棋盘格高），就会算错。</li>
</ul>
</li>
</ol>
<p>核心贡献</p>
<ul>
<li>痛点：用“有限2D+物理参考”解决了传统3D重建依赖多图/特殊硬件的问题，让普通人用手机拍图也能精准估食品份量；</li>
<li>标杆：三个方法覆盖了不同场景（多视角/单视角），提供了可复现的 pipeline，还对比了传统方法和GPT-4o，明确了优势；</li>
<li>应用：直接对接膳食评估，不管是个人健康管理（比如减肥时算热量），还是临床营养（比如糖尿病患者控食），都能用。</li>
</ul>
<p>未来方向</p>
<ul>
<li>自动化：去掉手动步骤，比如自动分割棋盘格、自动调缩放因子；</li>
<li>泛化：测复杂场景（乱背景、复杂食品），让方法更接地气；</li>
<li>融合：把3D重建和营养估算结合（比如知道体积后，自动算蛋白质/碳水），形成“拍图→算体积→算营养”的闭环。</li>
</ul>
<h1 id="我的一些疑问"><a href="#我的一些疑问" class="headerlink" title="我的一些疑问"></a>我的一些疑问</h1><p>我的问题是：1.这篇文章的贡献到底是什么？因为他们所使用的数据集是现成的、方法也是使用的别人的方法？是不是别人的方法本来不能做这个重建，然后他们（作者）对这个模型进行微调使得可以去进行食品数据的体积重建呢？此外这个文章还有什么实质性的贡献呢？2.我认为测试的数据量比较少。不足以说明有效性。这里只有20个类别，图片数量非常少（一共多少张）3.摘要我就觉得写的不好，关于具体的方法描写、没有；对于实验的结果数据、没有；感觉比较空。  </p>
<p>一、贡献定位模糊，创新性与实质性严重不足</p>
<ol>
<li>方法复用缺乏核心创新，未体现技术突破</li>
</ol>
<p>论文采用MetaFood Workshop挑战赛的现有Top方法（VolETA、ININ-VIAUN、FoodRiddle），但未明确说明对这些方法的实质性改造：</p>
<ul>
<li>未提供证据表明原始方法无法处理食品重建任务，亦无架构调整、损失函数优化、模块集成等创新设计的描述，仅提及“复用流程”（如VolETA调用NeuS2、One-2-3-45），与“直接套用现成方法”无本质区别；</li>
<li>论文统一采用 “棋盘格物理参考”（每格 1.2cm 已知），为三种方法设计了不同的尺度校准流程：<ul>
<li>VolETA：先手动用 MeshLab 测棋盘格网格初定缩放因子，再用深度图算食品宽 / 长 / 高微调，确保与真实尺寸一致；</li>
<li>ININ-VIAUN：通过角点投影匹配，计算棋盘格相邻角点的 3D 距离，用已知边长（1.2cm）反推缩放因子；</li>
<li>FoodRiddle：假设食品与棋盘格在同一平面，通过重投影对齐网格与真实尺寸。</li>
</ul>
</li>
<li>未区分“方法应用”与“方法创新”：领域内实质性贡献多体现为适配场景的方法优化（如SimpleFood45提出“三维模型+物理参照”的轻量级框架），而本文仅做方法复现，未解决食品重建的特有难题（如软质食品形变、多食物遮挡）。</li>
</ul>
<p>针对“是否微调模型适配食品数据”的疑问，论文存在关键信息缺失：</p>
<ul>
<li>未提及任何模型微调的实验细节（如微调数据集划分、学习率设置、微调轮次），亦无“原始模型vs微调后”的性能对比，无法证明“适配性改进”的存在；</li>
<li>核心流程中仅提到“手动调整缩放因子”“后处理补洞”等工程化操作，此类常规处理不属于学术意义上的“方法创新”，更未解决食品体积重建的核心瓶颈（如尺度校准精度、单视角几何推断）。</li>
</ul>
<p>对通用方法易出现特征提取不足或重建不完整。作者针对性优化：</p>
<ul>
<li>VolETA：加入 “关键帧选择” 模块，用高斯模糊检测 + 感知哈希过滤模糊 / 重复帧（仅保留 34.8% 有效帧），结合 SuperPoint/SuperGlue 增强弱纹理场景的特征点匹配；</li>
<li>ININ-VIAUN：对多视图数据同时测试 COLMAP、DiffusioNeRF、NeRF2Mesh 三种重建方案，选择视觉 / 几何最优结果，解决不同食品的重建适配性问题；</li>
<li>FoodRiddle：多视图重建中用 “2D 高斯溅射 + 泊松表面重建” 补全食品底部（如披萨下方被盘子遮挡的部分），避免体积漏算。</li>
</ul>
<p>二、数据规模与实验设计缺陷，有效性证明严重不足</p>
<h3 id="2-实验设计存在系统性缺陷，结果可信度低"><a href="#2-实验设计存在系统性缺陷，结果可信度低" class="headerlink" title="2. 实验设计存在系统性缺陷，结果可信度低"></a>2. 实验设计存在系统性缺陷，结果可信度低</h3><ul>
<li><strong>难度分级逻辑矛盾</strong>：“Hard级仅1张图”的设计极端化，现实场景中即使单视角拍摄也可能存在不同光照/角度差异，该设置无法模拟真实使用场景；</li>
<li><strong>缺乏关键验证环节</strong>：未采用交叉验证（领域内小样本实验的标准操作），仅简单划分固定测试集，无法排除数据划分偶然性导致的性能偏差；</li>
<li><strong>对比基线不完整</strong>：未与领域经典方法（如基于立体视觉的FoodVolumeNet、基于深度图的Nutrition3D）直接对比，仅对比挑战赛同批方法，无法凸显性能优势。</li>
</ul>
<p>三、摘要撰写不符合学术规范，信息传递效率极低</p>
<ol>
<li>方法描述模糊化，核心技术路径缺失</li>
</ol>
<p>摘要未提及任何关键技术细节：</p>
<ul>
<li>未明确“有限2D图像+物理参考”的具体实现方式（如是否用棋盘格校准、相机姿态估计方法、3D重建核心模型）；</li>
<li>未说明多视角与单视角场景的技术差异，仅泛谈“重建3D模型”，与领域内摘要的方法清晰度（如“提出结合SfM与NeuS2的两阶段重建框架”）差距显著。</li>
</ul>
<ol>
<li>摘要未呈现任何核心实验指标：</li>
</ol>
<ul>
<li>未提及体积估算精度（如MAPE、MAE）、形状相似度（如Chamfer距离）等关键数值，无法证明方法有效性；</li>
<li>未对比现有方法的性能提升（如“较GPT-4o降低24个百分点MAPE”此类核心结论未在摘要体现），读者无法快速判断成果价值。</li>
</ul>
<p>四、实验上稳定性分析不足：仅给出标准差数据，但未分析误差分布特征（如是否集中于某类食品/某拍摄角度），无法指导实际场景应用。</p>
<h3 id="2-局限性认知回避核心问题"><a href="#2-局限性认知回避核心问题" class="headerlink" title="2. 局限性认知回避核心问题"></a>2. 局限性认知回避核心问题</h3><p>论文提及的“手动步骤多”“未测复杂场景”等局限性，未触及根本缺陷：</p>
<ul>
<li>未承认“方法复用导致的泛化性不足”；</li>
<li>未提及“小样本训练导致的模型鲁棒性差”，回避数据量不足的核心问题。</li>
</ul>
<h3 id="3-应用落地衔接薄弱"><a href="#3-应用落地衔接薄弱" class="headerlink" title="3. 应用落地衔接薄弱"></a>3. 应用落地衔接薄弱</h3><p>未解决膳食评估的实际需求：</p>
<ul>
<li>未将3D体积与营养估算衔接（如未调用FNDDS数据库映射热量），脱离“食品计算服务健康管理”的核心目标；</li>
<li>未考虑移动端部署可行性（如模型参数量、推理速度未提及），实用价值有限。</li>
</ul>
<h2 id="总结与核心改进建议"><a href="#总结与核心改进建议" class="headerlink" title="总结与核心改进建议"></a>总结与核心改进建议</h2><ol>
<li><strong>明确创新定位</strong>：若侧重方法适配，需补充“原始模型在食品数据上的性能瓶颈”“微调/模块改进的具体方案”及消融实验；若侧重数据集分析，需扩展子集规模、补充专属标注并分析数据特性。</li>
<li><strong>扩充数据与优化设计</strong>：至少覆盖50类食品、1000张以上多视角图像，采用5折交叉验证，补充复杂场景（乱背景、多食品堆叠）测试。</li>
<li><strong>重构摘要与分析逻辑</strong>：摘要需包含“方法核心模块+关键指标数值+性能提升幅度”；实验部分增加经典基线对比与误差归因分析。</li>
<li><strong>强化应用导向</strong>：衔接营养估算流程，测试移动端部署性能，提升研究的实用价值。</li>
</ol>
<p>当前版本的论文在创新性、数据有效性与学术规范性上均存在严重缺陷，未达到领域发表标准，需进行实质性重构。</p>
<h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p>This paper presents three 3D food reconstruction methods (VolETA, ININ-VIAUN, FoodRiddle) developed by winning teams of the MetaFood Workshop Challenge, aiming to address accurate food portion estimation via real-scale 3D modeling from limited 2D inputs (single/multi-view images). These methods leverage physical references (e.g., checkerboards) for metric scale calibration and evaluate performance on 20 food items from the MetaFood3D dataset. However, the work has limitations as followings.</p>
<ol>
<li><strong>Abstract and key descriptions are overly vague</strong>: The abstract lacks specific method steps (e.g., no mention of key techniques like NeuS2 or ZeroNVS) and quantifiable results (e.g., no MAPE or Chamfer Distance values), failing to communicate the work’s core achievements.</li>
<li><strong>Small and limited test dataset</strong>: Only 20 food items are evaluated, and 2 (steak, chicken nugget) are excluded—sample diversity is insufficient (e.g., no liquid food like soup, no deformable food like pasta). This limits the generalizability of the results to real-world dietary scenarios.</li>
<li><strong>Failure to extend to dietary assessment</strong>: The paper claims to support dietary monitoring but does not conduct any experiments on practical nutrition analysis (e.g., converting volume to calories via food density, integrating with dietary record platforms). It remains at the “3D reconstruction” stage without linking to the ultimate application.</li>
<li><strong>Missing core nutritional annotations</strong>: It relies solely on 3D scanned volume as ground truth but ignores MetaFood3D’s original nutritional annotations (e.g., weight, calories, protein, fat). Volume is only an intermediate metric for dietary assessment—without these annotations, the work cannot demonstrate value for nutrition science or clinical use.</li>
<li><strong>Limited innovation (resembles method compilation)</strong>: The paper summarizes three challenge-winning methods but does not propose a novel unified framework, new reconstruction techniques, or improved scaling strategies. Its structure and focus are closer to a “challenge report” than original research.</li>
</ol>
</div><div class="post-end"><div class="post-prev"><a href="/2025/10/10/2025-10-10/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2025/10/07/2025-10-7/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#Psychically-Informed-3D-Food-Reconstruction-Methods-and-Results"><span class="toc-content-number">1.</span> <span class="toc-content-text">Psychically Informed 3D Food Reconstruction:  Methods and Results</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E8%83%8C%E6%99%AF%EF%BC%9A%E4%B9%8B%E5%89%8D%E7%9A%843D%E9%87%8D%E5%BB%BA%E6%96%B9%E6%B3%95"><span class="toc-content-number">1.1.</span> <span class="toc-content-text">背景：之前的3D重建方法</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-content-number">1.2.</span> <span class="toc-content-text">数据集</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95"><span class="toc-content-number">1.3.</span> <span class="toc-content-text">核心方法</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#1-VolETA%EF%BC%9A%E2%80%9C%E5%85%A8%E8%83%BD%E5%86%A0%E5%86%9B%E2%80%9D%EF%BC%8C%E4%BD%93%E7%A7%AF%E4%BC%B0%E7%AE%97%E6%9C%80%E5%87%86"><span class="toc-content-number">1.3.1.</span> <span class="toc-content-text">1. VolETA：“全能冠军”，体积估算最准</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E4%BA%AE%E7%82%B9%EF%BC%9A"><span class="toc-content-number">1.3.1.1.</span> <span class="toc-content-text">亮点：</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#2-ININ-VIAUN%EF%BC%9A%E2%80%9C%E7%A8%B3%E6%89%8E%E7%A8%B3%E6%89%93%E2%80%9D%EF%BC%8C%E9%9D%A0%E5%90%8E%E5%A4%84%E7%90%86%E8%A1%A5%E7%9F%AD%E6%9D%BF"><span class="toc-content-number">1.3.2.</span> <span class="toc-content-text">2. ININ-VIAUN：“稳扎稳打”，靠后处理补短板</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E4%BA%AE%E7%82%B9%E4%B8%8E%E4%B8%8D%E8%B6%B3%EF%BC%9A"><span class="toc-content-number">1.3.2.1.</span> <span class="toc-content-text">亮点与不足：</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#3-FoodRiddle%EF%BC%9A%E2%80%9C%E5%BD%A2%E7%8A%B6%E4%B8%93%E5%AE%B6%E2%80%9D%EF%BC%8C3D%E6%A8%A1%E5%9E%8B%E6%9C%80%E5%83%8F%E7%9C%9F%E5%AE%9E%E9%A3%9F%E5%93%81"><span class="toc-content-number">1.3.3.</span> <span class="toc-content-text">3. FoodRiddle：“形状专家”，3D模型最像真实食品</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E4%BA%AE%E7%82%B9%EF%BC%9A-1"><span class="toc-content-number">1.3.3.1.</span> <span class="toc-content-text">亮点：</span></a></li></ol></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-content-number">1.4.</span> <span class="toc-content-text">实验结果</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#1-%E4%BD%93%E7%A7%AF%E7%B2%BE%E5%BA%A6%EF%BC%88MAPE%E8%B6%8A%E4%BD%8E%E8%B6%8A%E5%A5%BD%EF%BC%89"><span class="toc-content-number">1.4.1.</span> <span class="toc-content-text">1. 体积精度（MAPE越低越好）</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#2-%E5%BD%A2%E7%8A%B6%E7%B2%BE%E5%BA%A6%EF%BC%88Chamfer%E8%B7%9D%E7%A6%BB%E8%B6%8A%E5%B0%8F%E8%B6%8A%E5%A5%BD%EF%BC%89"><span class="toc-content-number">1.4.2.</span> <span class="toc-content-text">2. 形状精度（Chamfer距离越小越好）</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E4%B8%80%E4%BA%9B%E8%AE%A8%E8%AE%BA"><span class="toc-content-number">1.5.</span> <span class="toc-content-text">一些讨论</span></a></li></ol></li><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#%E6%88%91%E7%9A%84%E4%B8%80%E4%BA%9B%E7%96%91%E9%97%AE"><span class="toc-content-number">2.</span> <span class="toc-content-text">我的一些疑问</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#2-%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1%E5%AD%98%E5%9C%A8%E7%B3%BB%E7%BB%9F%E6%80%A7%E7%BC%BA%E9%99%B7%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%8F%AF%E4%BF%A1%E5%BA%A6%E4%BD%8E"><span class="toc-content-number">2.0.1.</span> <span class="toc-content-text">2. 实验设计存在系统性缺陷，结果可信度低</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#2-%E5%B1%80%E9%99%90%E6%80%A7%E8%AE%A4%E7%9F%A5%E5%9B%9E%E9%81%BF%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="toc-content-number">2.0.2.</span> <span class="toc-content-text">2. 局限性认知回避核心问题</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#3-%E5%BA%94%E7%94%A8%E8%90%BD%E5%9C%B0%E8%A1%94%E6%8E%A5%E8%96%84%E5%BC%B1"><span class="toc-content-number">2.0.3.</span> <span class="toc-content-text">3. 应用落地衔接薄弱</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E6%A0%B8%E5%BF%83%E6%94%B9%E8%BF%9B%E5%BB%BA%E8%AE%AE"><span class="toc-content-number">2.1.</span> <span class="toc-content-text">总结与核心改进建议</span></a></li></ol></li><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#Review"><span class="toc-content-number">3.</span> <span class="toc-content-text">Review</span></a></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2025 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>