<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="华为机考计算题-概率&#x2F;矩阵若二次型 $f(x_1,x_2,x_3)&#x3D;5x_{1}^2+5x_{2}^2+cx_3^2−2x_1x_2+6x_1x_3−6x_2x_3$的秩为 2 ,则 c&#x3D;?   f &#x3D; 5x_1^2 + 5x_2^2 + cx_3^2 - 2x_1x_2 + 6x_1x_3 - 6x_2x_3对应的矩阵是：  A&#x3D;\begin{pmatrix}5 &amp; -1 &amp; 3 \\ -1 &amp;">
<meta property="og:type" content="article">
<meta property="og:title" content="2026-1-9">
<meta property="og:url" content="http://example.com/2026/01/09/2026-1-9/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="华为机考计算题-概率&#x2F;矩阵若二次型 $f(x_1,x_2,x_3)&#x3D;5x_{1}^2+5x_{2}^2+cx_3^2−2x_1x_2+6x_1x_3−6x_2x_3$的秩为 2 ,则 c&#x3D;?   f &#x3D; 5x_1^2 + 5x_2^2 + cx_3^2 - 2x_1x_2 + 6x_1x_3 - 6x_2x_3对应的矩阵是：  A&#x3D;\begin{pmatrix}5 &amp; -1 &amp; 3 \\ -1 &amp;">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2026-01-09T03:05:15.000Z">
<meta property="article:modified_time" content="2026-01-20T10:33:58.926Z">
<meta property="article:author" content="Schwertlilien">
<meta property="article:tag" content="面经">
<meta name="twitter:card" content="summary"><title>2026-1-9 - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>2026-1-9</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2026-01-09</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2026-01-20</time></div>

<div class="post-tags"><i class="icon fa-solid fa-tags" title="标签"></i><a class="post-tag" href="/tags/%E9%9D%A2%E7%BB%8F/">面经</a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约2.6K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><h1 id="华为机考"><a href="#华为机考" class="headerlink" title="华为机考"></a>华为机考</h1><h2 id="计算题-概率-矩阵"><a href="#计算题-概率-矩阵" class="headerlink" title="计算题-概率/矩阵"></a>计算题-概率/矩阵</h2><p><strong>若二次型 $f(x_1,x_2,x_3)=5x_{1}^2+5x_{2}^2+cx_3^2−2x_1x_2+6x_1x_3−6x_2x_3$的秩为 2 ,则 c=?</strong></p>
<blockquote>
<script type="math/tex; mode=display">
f = 5x_1^2 + 5x_2^2 + cx_3^2 - 2x_1x_2 + 6x_1x_3 - 6x_2x_3</script><p>对应的矩阵是：</p>
<script type="math/tex; mode=display">
A=\begin{pmatrix}5 & -1 & 3 \\ -1 & 5 & -3 \\ 3 & -3 & c \end{pmatrix}</script><p>秩为2=行列式=0:</p>
<script type="math/tex; mode=display">
|A| = 24(c - 3)\\
24(c - 3) = 0 \\
 c = 3</script></blockquote>
<p><strong>设 $A$为3阶矩阵，且 $∣A∣=2$ ，则$\vert(\frac 1 3 A)^{-1}-\frac 1 2A^*\vert=?$</strong></p>
<p><strong>注：$A^∗$表示 $A$ 的伴随矩阵</strong></p>
<blockquote>
<p>逆的缩放律：</p>
<script type="math/tex; mode=display">
(\alpha A)^{-1} = \frac{1}{\alpha} A^{-1}</script><p>所以：</p>
<script type="math/tex; mode=display">
\left(\frac{1}{3}A\right)^{-1} = 3A^{-1}</script><p>A−1=∣A∣1A∗=21A∗</p>
<p>代入：</p>
<script type="math/tex; mode=display">
3A^{-1} = \frac{3}{2}A^*</script><script type="math/tex; mode=display">
\frac{3}{2}A^* - \frac{1}{2}A^* = A^*</script><p>若 $A$ 为 $n \times n$ 矩阵，则：</p>
<script type="math/tex; mode=display">
|A^*| = |A|^{n-1}</script><p>这里 $n = 3$，$|A| = 2$，因此：</p>
<script type="math/tex; mode=display">
|A^*| = 2^{3-1} = 4</script></blockquote>
<p><strong>设 A 是 3 阶方阵，将 A 的第 1 列与第 2 列交换得到 B，再把 B 的第 2 列加到第 3 列得到C，则满足 AQ=C 的可逆矩阵 Q 为?</strong></p>
<blockquote>
<script type="math/tex; mode=display">
\begin{pmatrix}c_1&c_2&c_3\end{pmatrix}
\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{pmatrix}=\begin{pmatrix}c_2&c_1&c_1+c_3\end{pmatrix}</script></blockquote>
<p><strong>向量 A=[1,2,3,0,−3]的 L1范数为?</strong></p>
<blockquote>
<script type="math/tex; mode=display">
\| A\|_1=\sum_i |a_i|=9\\</script></blockquote>
<p><strong>某12层Transformer，hidden size=768，MLP 扩展比=4，则每层 MLP 的参数量约为？</strong></p>
<ul>
<li>[ ] 18.88 M</li>
<li>[ ] 9.44 M</li>
<li>[x] 4.72 M</li>
<li>[ ] 2.36 M</li>
</ul>
<blockquote>
<p>12层是个无效信息。因为问的是每层的参数量。对于一个transformer(encoder+decoder),会有两个ffn网络。<strong>NO!</strong></p>
<p><strong>Actually: </strong>这里有一个<strong>行业默认前提</strong>，题目虽然没写，但在 ML 题库里几乎是<strong>约定俗成的</strong>：<strong>“12 层 Transformer” = 12 个 Transformer block（encoder blocks）</strong></p>
<p>MLP 结构：</p>
<script type="math/tex; mode=display">
768 \rightarrow 3072 \rightarrow 768</script><p>参数量：</p>
<ul>
<li>第一层：$768 \times 3072 = 2.36M$</li>
<li>第二层：$3072 \times 768 = 2.36M$</li>
</ul>
<p>合计：</p>
<script type="math/tex; mode=display">
2.36 + 2.36 = 4.72M</script><p>（bias 通常忽略）</p>
</blockquote>
<p>A∈R2×2，特征值：</p>
<script type="math/tex; mode=display">
\lambda_1 = 3,\quad \lambda_2 = -1</script><p>求：$\mathrm{tr}(A)$</p>
<blockquote>
<script type="math/tex; mode=display">
tr(A)=\sum_i \lambda_i=2</script></blockquote>
<p><strong>用梯形法则计算定积分时，若被积函数的二阶导数在区间 [a,b]上的最大值为M，积分区间等分为 n 段，则截断误差的上界为以下哪一项?</strong></p>
<ul>
<li>[ ] $\frac{M(b-a)^2}{2n^2}$</li>
<li>[x] $\frac{M(b-a)^3}{12n^2}$</li>
<li>[ ] $\frac{M(b-a)}{2n}$</li>
<li>[ ] $\frac{M(b-a)^4}{24n^3}$</li>
</ul>
<blockquote>
<p>复合梯形公式的截断误差满足：</p>
<script type="math/tex; mode=display">
|E_T| \le \frac{(b-a)^3}{12n^2}\max_{x\in[a,b]}|f''(x)|</script><p>A：少了一个 $(b-a)$，量纲不对</p>
<p>C：这是一阶误差，不可能</p>
<p>D：这是 Simpson 法（四阶）才会出现的形式</p>
</blockquote>
<p><strong>在一个基于Transformer的多模态模型中，视觉-文本对齐模块的目标是将图像特征 $V\in \mathbb{R}^{n×d_v}$和文本特征$T\in\mathbb{R}^{n、times d_t}$映射到同一语义空间若采用对比学习(Contrastive Learning)框架，则损失函数可选下列哪个?</strong></p>
<ul>
<li>[x] $-\log \frac{\exp(\text{sim}(V_i,T_i))}{\sum_j \exp(\text{sim}(V_i,T_j))}$</li>
<li>[ ] $\text{CrossEntropy}(V_i,T_j)$</li>
<li>[ ] $\text{KL-Divergence}(V_i|T_j)$</li>
<li>[ ] $\text{MSE}(V_i,T_j)$</li>
</ul>
<blockquote>
<ul>
<li>使用 <strong>InfoNCE / NT-Xent</strong> 形式</li>
<li>即：</li>
</ul>
<script type="math/tex; mode=display">
-\log \frac{\exp(\text{sim}(V_i,T_i))}{\sum_j \exp(\text{sim}(V_i,T_j))}</script><p><strong>选项判断</strong></p>
<ul>
<li>A：<strong>标准 InfoNCE 形式（只是用 sin 表示相似度）</strong></li>
<li>B：普通 CrossEntropy，未体现“负样本对比”</li>
<li>C：KL 用于分布对齐，不是主流 CL</li>
<li>D：MSE 不具备判别性</li>
</ul>
</blockquote>
<h2 id="单选题"><a href="#单选题" class="headerlink" title="单选题"></a>单选题</h2><p><strong>在处理用户评论情感分类任务时，你发现数据集里正面评论和负面评论的比例是9:1，这是一个典型的数据不平衡问题。在这种情况下，以下哪个评估指标最不能客观地反映模型的性能?</strong></p>
<ul>
<li>[ ] 召回率(Recall)</li>
<li>[ ] AUC(ROC曲线下面积)</li>
<li>[ ] F1-Score</li>
<li>[x] 准确率(Accuracy)</li>
</ul>
<blockquote>
<p>在正负样本比例严重不平衡的数据集中（如正负样本比为 9：1）：</p>
<ul>
<li>如果一个模型把所有样本都预测为多数类，准确率仍然会很高；</li>
<li>但这种预测实际上对少数类没有区分能力；</li>
<li>因此准确率无法有效反映模型在不平衡数据集上的性能。</li>
</ul>
<p>而像 <strong>Recall、F1-score、AUC</strong> 这些指标对不平衡更敏感，更能体现模型性能。</p>
<p>所以本题选择 <strong>D. 准确率（Accuracy）</strong> 作为不适合评估不平衡数据集性能的指标。</p>
</blockquote>
<p><strong>大模型训练中的”LoRA(Low-Rank Adaptation)“技术主要作用是？</strong></p>
<blockquote>
<p> 在微调阶段通过低秩矩阵减少要更新的参数量。</p>
<p>LoRA 的核心思想是：</p>
<script type="math/tex; mode=display">
W \leftarrow W + \Delta W, \quad \Delta W = BA</script><p>其中：$A \in \mathbb{R}^{r \times d}$；$B \in \mathbb{R}^{d \times r}$；$r \ll d$；</p>
<p><strong>特点</strong>：</p>
<ul>
<li>冻结原模型参数</li>
<li>只训练低秩矩阵</li>
<li>显著减少显存与训练成本</li>
</ul>
</blockquote>
<p><strong>模型出现过拟合，哪些操作和优化能够缓解或减少过拟合带来的问题？</strong></p>
<ul>
<li>[ ] 在原有数据集上重新训练</li>
<li>[x] 添加正则化</li>
<li>[ ] 将数据集复制后再训练</li>
<li>[ ] 减少数据集后再训练</li>
</ul>
<p><strong>在机器学习中，将一张 28 × 28 像素的灰度图像转换为特征向量时，要求特征向量能完整保存原有信息的操作是?</strong></p>
<ul>
<li>[ ] 仅保留边缘检测后的像素值</li>
<li>[ ] 计算每个4×4 块的平均值，生成49 维向量</li>
<li>[x] 将像素矩阵按行展平为 784 维向量</li>
<li>[ ] 直接使用原始像素矩阵作为输入</li>
</ul>
<blockquote>
<p>我本来觉得应该是原始的像素矩阵输入，但是这并不是一个“向量”</p>
</blockquote>
<p><strong>在参数高效微调(PEFT)方法中，LoRA 通过在 Transformer 线性层插入可训练的低秩矩阵来近似权重更新。下面哪一项最能解释为什么 LoRA 对推理时延几平没有负面影响？</strong></p>
<ul>
<li>[ ] LoRA 把权重量化为 INT4</li>
<li>[ ] 低秩更新的秩很小，且与原矩阵无法并行融合</li>
<li>[ ] LoRA 只在训练阶段生效，推理阶段完全丢弃</li>
<li>[x] 低秩矩阵的乘法可以离线编译到权重中</li>
</ul>
<blockquote>
<p>A：那是量化，不是LoRA</p>
<p>B：可以融合</p>
<p>C：LoRA用于推理</p>
</blockquote>
<p><strong>二分法(Bisection Method)求解方程 时，其收敛速度是()?</strong></p>
<ul>
<li>[ ] 超线性收敛</li>
<li>[ ] 不收敛</li>
<li>[ ] 二次收敛</li>
<li>[ ] 线性收敛</li>
</ul>
<blockquote>
<p><strong>结论</strong></p>
<ul>
<li>二分法每次区间长度减半</li>
<li>误差：$|e_k| \le \frac{b-a}{2^k}$</li>
</ul>
<p><strong>因此</strong></p>
<ul>
<li>收敛阶：<strong>线性收敛（order 1）</strong></li>
</ul>
</blockquote>
<h2 id="多选题"><a href="#多选题" class="headerlink" title="多选题"></a>多选题</h2><p><strong>以下说法正确的是（ ）</strong></p>
<ul>
<li><p>[ ] 向量组 $\alpha_1,\alpha_2,\dots,\alpha_n$ 线性相关的充要条件是：任意一个向量 $\alpha_i$ 都可以由其余 $n-1$ 个向量线性表示。</p>
</li>
<li><p>[x] $n \ge 2$，向量组$\alpha_1-\alpha_2,\ \alpha_2-\alpha_3,\ \dots,\ \alpha_{n-1}-\alpha_n,\ 2(\alpha_n-\alpha_1)$一定线性相关。</p>
</li>
<li><p>[x] 设 $A,B$ 为满足 $AB=0$ 的任意两个非零矩阵，则一定有A 的列向量线性相关，B 的行向量线性相关。</p>
</li>
<li><p>[ ] 设 $A$ 为 3 阶非零实方阵，$A^<em>$ 为伴随矩阵，若$A^</em> = -A^T$则 $\det(A) &lt; 0$</p>
</li>
</ul>
<blockquote>
<p>A. 这不是线性相关的<strong>等价定义</strong>:至少存在一向量可由其余向量线性表示 ⇔ 向量组线性相关</p>
<p>B. $(\alpha_1-\alpha_2)+\cdots+(\alpha_{n-1}-\alpha_n)+2(\alpha_n-\alpha_1)=0$：存在非零线性组合等于零 → 线性相关</p>
<p>C. $AB=0$​ 只说明 <strong>B 的值域 ⊆ A 的零空间</strong>。并不必然导致：A 列向量线性相关、B 行向量线性相关</p>
<p>D. $\det(A^*) = \det(A)^{n-1} = \det(A)^2$</p>
<p>$\det(-A^T)=(-1)^3\det(A)=-\det(A)$</p>
<p>$\Rightarrow \det(A)^2 = -\det(A) \Rightarrow \det(A)=-1\le 0$</p>
</blockquote>
<p><strong>主成分分析（PCA）的前 (k) 个主成分具有哪些性质？</strong></p>
<ul>
<li>[x] 方差依次最大</li>
<li>[ ] 原始数据旋转后主成分不变</li>
<li>[x] 主成分之间正交</li>
<li>[ ] 与原始特征线性无关</li>
</ul>
<blockquote>
<p>A. PCA 的目标函数：最大化投影方差。第 1 主成分方差最大，第 2 次之，依次递减</p>
<p>B. PCA <strong>不具有旋转不变性</strong>，数据旋转会改变协方差矩阵 → 改变主成分方向</p>
<p>C. 主成分是协方差矩阵的特征向量，对应不同特征值的特征向量正交</p>
<p>D. 主成分 <strong>是原始特征的线性组合</strong>，恰恰“线性相关”</p>
</blockquote>
<p><strong>在为大型语言模型（LLM）选择与调整优化器时，以下哪些论断在理论或工程实践中被广泛认可？</strong></p>
<ul>
<li>[x] Adam 的 Bias Correction 用于修正训练初期动量估计偏小问题，有助于更合理的学习率尺度</li>
<li>[x] 在“峡谷地形”中，相较于 SGD，Adam / RMSprop 可自适应缩放不同维度学习率，抑制振荡、加速收敛</li>
<li>[x] 对于 Adam / RMSprop，若某参数梯度长期稀疏，则其有效学习率会逐渐减小，从而更新更稳定</li>
<li>[x] 相比 SGD with Momentum，Adam / RMSprop 需要维护额外的一阶、二阶动量，显著增加显存占用</li>
</ul>
<blockquote>
<p>A. Adam:$\hat m_t = \frac{m_t}{1-\beta_1^t},\quad \hat v_t = \frac{v_t}{1-\beta_2^t}$, 对大模型早期训练非常关键</p>
<p>B. Adam/RMSprop 按维度归一化梯度，是其最经典的优势场景</p>
<p>C. 累积二阶矩$v_t$会抑制噪声更新，稀疏梯度下表现稳定</p>
<p>D. Adam 需要：参数\一阶动量 $m_t$\二阶动量 $v_t$，显存占用约为 SGD 的 2–3 倍</p>
</blockquote>
<p>在某场景中，事件$A_1,A_2,A_3$构成样本空间的一个划分。满足$P(A_1)=0.2,\ P(A_2)=0.3,\ P(A_3)=0.5$。已知条件概率$P(B|A_1)=0.1,\ P(B|A_2)=0.4,\ P(B|A_3)=0.6$。下面哪些是正确的？</p>
<ul>
<li>[ ] $P(A_2|B)\neq P(B|A_2)$</li>
<li>[x] $P(B)=0.44$</li>
<li>[x] $P(A_1|B)\approx0.045$</li>
<li>[x] $P(A_2|B)\approx0.273$</li>
</ul>
<blockquote>
<p>总概率：</p>
<script type="math/tex; mode=display">
P(B)=0.2\times0.1+0.3\times0.4+0.5\times0.6=0.44</script><p>贝叶斯公式：</p>
<script type="math/tex; mode=display">
P(A_2|B)=\frac{P(B|A_2)P(A_2)}{P(B)}=\frac{0.4\times0.3}{0.44}\approx0.273\\
P(A_1|B)=\frac{0.1\times0.2}{0.44}\approx0.045</script></blockquote>
<p><strong>以下优化算法，说法正确的是？</strong></p>
<ul>
<li>[x] L-BFGS 是二阶优化方法</li>
<li>[ ] Momentum 是二阶优化方法</li>
<li>[x] Adam 是一阶优化方法</li>
<li>[x] AdaGrad 是一阶优化方法</li>
<li>[x] AdamW 是一阶优化方法</li>
</ul>
<blockquote>
<p>A:使用 Hessian 的低秩近似,属于拟牛顿法（二阶） q</p>
<p>B:Momentum 仅使用一阶梯度 + 指数加权,不涉及 Hessian 或其近似</p>
<p>C:仅依赖一阶梯度,二阶矩是梯度平方的统计量，不是 Hessian</p>
<p>E:AdamW = Adam + decoupled weight decay,本质仍是一阶方法</p>
</blockquote>
</div><div class="post-end"><div class="post-prev"><a href="/2026/01/17/2026-1-17/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2026/01/05/2026-1-5/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#%E5%8D%8E%E4%B8%BA%E6%9C%BA%E8%80%83"><span class="toc-content-number">1.</span> <span class="toc-content-text">华为机考</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E8%AE%A1%E7%AE%97%E9%A2%98-%E6%A6%82%E7%8E%87-%E7%9F%A9%E9%98%B5"><span class="toc-content-number">1.1.</span> <span class="toc-content-text">计算题-概率&#x2F;矩阵</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E5%8D%95%E9%80%89%E9%A2%98"><span class="toc-content-number">1.2.</span> <span class="toc-content-text">单选题</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E5%A4%9A%E9%80%89%E9%A2%98"><span class="toc-content-number">1.3.</span> <span class="toc-content-text">多选题</span></a></li></ol></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2026 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>